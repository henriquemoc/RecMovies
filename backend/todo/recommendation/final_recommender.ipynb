{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "67e93cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "   Imports needed\n",
    "\"\"\"\n",
    "import pandas as pd\n",
    "import sqlite3\n",
    "import numpy as np\n",
    "import gensim\n",
    "from nltk.corpus import stopwords\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "813d4a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextProcessor():\n",
    "  \"\"\"\n",
    "    Class responsible for treating the dataset and training the LDA model.\n",
    "  \"\"\"\n",
    "\n",
    "  def tokenize(self, df_overviews):\n",
    "      \"\"\"\n",
    "      Tokenize the dataset. Return a list of str list, where the latter represent the list of words of each overview.\n",
    "      \n",
    "      \"\"\"\n",
    "      return [gensim.utils.simple_preprocess(overview, deacc=True) for overview in df_overviews]\n",
    "  \n",
    "  def construct_bigrams(self, tokenized_overview):\n",
    "      \"\"\"\n",
    "      Receives a list of str lists.\n",
    "      \n",
    "      Construct bigrams(two words terms), conservating the meaning of some sentences.\n",
    "      Ex: The bigram 'United States' is much more meaningful than the two unigrams 'United' and 'States'.\n",
    "      The same follows for inumerous terms.\n",
    "      \n",
    "      Returns the tokenized overview with bigrams.\n",
    "      \"\"\"\n",
    "                    #A bigram should appear at least 10 times in the dataset to be considered.\n",
    "      bg = gensim.models.Phrases(tokenized_overview, min_count=5, threshold=10) \n",
    "                                                        #Higher threshold means less bigrams.\n",
    "      bg_mod = gensim.models.phrases.Phraser(bg)\n",
    "\n",
    "      return [bg_mod[overview] for overview in tokenized_overview]\n",
    "  \n",
    "  def remove_english_stopwords(self, all_tokenized_overviews, additional_stopwords = []):\n",
    "      \"\"\"\n",
    "      Remove stopwords from dataset.\n",
    "      \n",
    "      \"\"\"\n",
    "\n",
    "      english_stopwords = stopwords.words(\"english\") + additional_stopwords\n",
    "      return [[word for word in overview if word not in english_stopwords] for overview in all_tokenized_overviews]\n",
    "  \n",
    "  def lemmatization(self, texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "      \"\"\"https://spacy.io/api/annotation\"\"\"        \n",
    "      \n",
    "      nlp = spacy.load(\"en_core_web_sm\", disable=['parser', 'ner'])\n",
    "\n",
    "      texts_out = []\n",
    "      for sent in texts:\n",
    "          doc = nlp(\" \".join(sent)) \n",
    "          texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "      return texts_out\n",
    "\n",
    "  def text_process(self,\n",
    "                    doc_set,\n",
    "                    extra_stop_words=[]):\n",
    "      \"\"\"\n",
    "      \n",
    "      Tokenize, remove stopwords, generate bigrams and lemmatize the dataset.\n",
    "      Returns the filtered dataset\n",
    "      \n",
    "      Parameters\n",
    "      -----------\n",
    "      doc_set: List of list of documents that will be treated.\n",
    "      extra_stop_words: Additional stopwords that will be used together with nltk.corpus stopwords.\n",
    "      \"\"\"\n",
    "      # generate a list of tokenized plots.\n",
    "      tokenized_overview = self.tokenize(doc_set)\n",
    "      # remove stopwords\n",
    "      ens = self.remove_english_stopwords(tokenized_overview, extra_stop_words)\n",
    "      # generate bigrams.\n",
    "      bigram_tokenized = self.construct_bigrams(ens)\n",
    "      # lemmatize dataset.\n",
    "      treated_dataset = self.lemmatization(bigram_tokenized)\n",
    "      \n",
    "      return treated_dataset\n",
    "      \n",
    "      \n",
    "\n",
    "  def create_corpus(self, texts):\n",
    "      # turn our tokenized documents into an id <-> term dictionary\n",
    "      dictionary = gensim.corpora.Dictionary(texts)\n",
    "      # Filtering words that appears in less than 1 and more than 80% of documents.\n",
    "      dictionary.filter_extremes(no_below=2, no_above=0.80)\n",
    "      dictionary.compactify()\n",
    "\n",
    "      # convert tokenized documents into a document-term matrix\n",
    "      corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "\n",
    "      return (corpus, dictionary)\n",
    "  \n",
    "  def generate_lda(self, corpus, dictionary, num_topics, epochs=200, passes=300, chunksize=90000):\n",
    "      \"\"\"\n",
    "      Generate LDA model and returns it.\n",
    "      \n",
    "      Parameters\n",
    "      -----------\n",
    "      corpus: tokenized documents in a document-term matrix. (Use create_corpus to generate it)\n",
    "      dictionary: auxiliar structure \n",
    "      num_topics : number of topics that will be created\n",
    "     \n",
    "      epochs: number of training iterations\n",
    "      \n",
    "      passes: number of times that each document will be utilized in a specific iteraiton\n",
    "      \n",
    "      chunksize: number of documents that will be considered in an iteration.\n",
    "      \n",
    "      Return\n",
    "      -------\n",
    "      \n",
    "      Trained LDA model.\n",
    "      \n",
    "      \"\"\"\n",
    "      model = gensim.models.ldamodel.LdaModel(corpus=corpus, \n",
    "                                              num_topics=num_topics, \n",
    "                                              iterations=epochs,\n",
    "                                              passes=passes,\n",
    "                                              chunksize=chunksize,\n",
    "                                              id2word=dictionary,\n",
    "                                              alpha=\"auto\")\n",
    "\n",
    "      return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a8c86384",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LDAContentBasedRecommender():\n",
    "  \"\"\"\n",
    "  Content Based recommender \n",
    "\n",
    "  \"\"\"\n",
    "  def __init__(self, model, matrix_document_topic, item_dictionary, default_k = 5, sim_method= None):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    -----------\n",
    "    model -> LDA or HDP model.\n",
    "    \n",
    "    matrix_document_topic -> Matrix that stores the distribution of each topic in the documents.\n",
    "    It can be obtained by model[corpus], if the model was calculated using gensim library. \n",
    "\n",
    "    item_dictionary -> Dictionary that maps the id with its position in the dataset\n",
    "    (must correspond with the corpus used to train the LDA model.)\n",
    "    \n",
    "    Optional\n",
    "    ---------\n",
    "    \n",
    "    default_k => Number of the closest items that will be consireded in the KNN evaluation.\n",
    "    \n",
    "    sim_method (functional type) => Similarity method. \n",
    "    If nothing is passed, then cossine will be used.\n",
    "    \n",
    "    \"\"\"\n",
    "    self.model = model\n",
    "    \n",
    "    self.corpus_lda = matrix_document_topic\n",
    "    \n",
    "    self.len = model.num_topics\n",
    "    \n",
    "    #Document distribution over topics.\n",
    "    self.item_matrix = self.__doc_x_topics() \n",
    "    \n",
    "    # ItemID <-> Position_in_the_matrix dictionary\n",
    "    self.item_dic = item_dictionary \n",
    "    \n",
    "    self.k = default_k \n",
    "    \n",
    "    self.sim_method = sim_method if sim_method != None else self.cossine\n",
    "    \n",
    "  def __doc_x_topics(self):    \n",
    "    \"\"\"\n",
    "    Generate the documents x topics matrix that will be used to calculate the scores.\n",
    "    \"\"\"\n",
    "    \n",
    "    item_matrix = gensim.similarities.MatrixSimilarity(self.corpus_lda).index\n",
    "    \n",
    "    return item_matrix\n",
    "  \n",
    "\n",
    "  def cossine(self, va, vb):\n",
    "    #Cossine between normalized vector and crude item vector.\n",
    "    cossine = np.dot(va,vb)/(np.linalg.norm(va)*np.linalg.norm(vb))\n",
    "    \n",
    "    return cossine   \n",
    "    \n",
    "    \n",
    "  def getScore(self, all_evaluations, item_vector):\n",
    "      \"\"\"\n",
    "        Predict the score of a target item utilizing the top K most similar components based on\n",
    "        the user's previous rated items.\n",
    "        \n",
    "        The calculation is made by adding the product of each one of the top K scores given by an user\n",
    "        and the similarity between the target item and the rated item vector, using the similarity method \n",
    "        passed in the method.\n",
    "        \n",
    "        Parameters\n",
    "        -----------\n",
    "        all_evaluation (dataframe) => dataframe that contains all the user's rated items.\n",
    "        \n",
    "        item_vector (np.array) => target item vector \n",
    "\n",
    "        Returns\n",
    "        --------\n",
    "        score (int) => Predicted score of the target item.\n",
    "      \n",
    "      \"\"\"\n",
    "      \n",
    "      score = 0\n",
    "      \n",
    "      #Get the similarity score with the chosen method between the target item and all the other\n",
    "      #items that a specific user consumed.\n",
    "      all_evaluations['Similarity'] = all_evaluations.apply(lambda x: \n",
    "        self.sim_method(item_vector,  self.item_matrix[self.item_dic[x['ItemId']], :]), \n",
    "        axis=1)\n",
    "      \n",
    "      #Sort the similarity in decreasing order.\n",
    "      all_evaluations.sort_values(by='Similarity', ascending=False, inplace=True)\n",
    "      \n",
    "      main_items = None\n",
    "      \n",
    "      #Get the top K items if the number of elements in the dataset is greater than the K. Else take all elements. \n",
    "      if len(all_evaluations) > self.k:\n",
    "        main_items = all_evaluations[0:self.k].to_numpy()\n",
    "      else:\n",
    "        main_items = all_evaluations.to_numpy()\n",
    "      \n",
    "      #The score of the item will be the sum of the ratings times the similarity of the predicted item and the top k consumed item.\n",
    "      for _, rating, similarity in main_items:\n",
    "        score += similarity*rating\n",
    "      \n",
    "      #Normalize the score\n",
    "      score /= len(main_items)   \n",
    "            \n",
    "      return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "123a3eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def treat_df(dataset):\n",
    "    dataset = dataset.drop(['poster_path','release_date','language'], axis=1)\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "\n",
    "def lda_training(dataset, number_of_topics_lda):\n",
    "   #Selecting columns to train LDA.\n",
    "   target_dataset = dataset[['title', 'genres']].copy()\n",
    "  \n",
    "   #Removing any N/A, NaN or None from these columns.\n",
    "   target_dataset = target_dataset.apply(lambda row: \" \".join(row.values.astype(str)) \\\n",
    "     .replace(\"N/A\", \"\").replace(\"NaN\", \"\").replace(\"None\", \"\"), axis=1)\n",
    "\n",
    "   tto = TextProcessor()\n",
    "\n",
    "   #Processing the text.\n",
    "   tokenized = tto.text_process(target_dataset)\n",
    "  \n",
    "   (corpus, dictionary) = tto.create_corpus(tokenized)\n",
    "\n",
    "   #Generates LDA Model.\n",
    "   lda_model = tto.generate_lda(corpus, dictionary, number_of_topics_lda)\n",
    "  \n",
    "   #Matrix that represents the distribution of topics from the documents.\n",
    "   lda_matrix = lda_model[corpus]\n",
    "  \n",
    "   return (lda_model, lda_matrix)\n",
    "\n",
    "\n",
    "def create_content_obj(content_dataframe):\n",
    "  number_of_topics = 50\n",
    "  \n",
    "  lda_model, lda_matrix = lda_training(content_dataframe, number_of_topics)\n",
    "  \n",
    "  #Item dictionary.\n",
    "  item_dictionary = {v: k for k,v in enumerate(content_dataframe['id'])}\n",
    "  \n",
    "  #Creating content Object\n",
    "  content_obj = LDAContentBasedRecommender(lda_model, lda_matrix, item_dictionary)  \n",
    "\n",
    "  return content_obj\n",
    "\n",
    "#Open dataframes\n",
    "dat = sqlite3.connect('/Users/matheuspradomiranda/TP-Eng-Soft1/backend/database/db.sqlite3')\n",
    "dataframe = pd.read_sql_query('SELECT * FROM todo_movies',dat)\n",
    "\n",
    "new_dataframe = treat_df(dataframe)\n",
    "\n",
    "content_obj = create_content_obj(new_dataframe)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "404e6675",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fromdf_todict(new_dataframe):\n",
    "    \"\"\"\n",
    "    Creating dicts with columns required for wr\n",
    "    \"\"\"\n",
    "    new_dict = new_dataframe[['popularity','vote_average','vote_count']].to_dict()\n",
    "    \n",
    "    return new_dict\n",
    "\n",
    "\n",
    "def calculate_score(similarity, item_data, i, df_mean, min_votes):\n",
    "    \"\"\"\n",
    "    Calculate the final score.\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    #(WR)=(v/(v+m))R+(m/(v+m))C      \n",
    "    wr = ((item_data['vote_count'][i]/(item_data['vote_count'][i]+min_votes))\\\n",
    "      *item_data['vote_average'][i]+(min_votes/(item_data['vote_count'][i]+min_votes)))*df_mean\n",
    "\n",
    "    final_score = 0.99*similarity + 0.01*wr \n",
    "    return final_score\n",
    "\n",
    "\n",
    "def movie_recommender(title, new_dict, position, content_vec):\n",
    "    \"\"\"\n",
    "    Recomend top-10 items.\n",
    "    \"\"\"\n",
    "    final_dict = {}\n",
    "    for i in range(len(new_dataframe)):\n",
    "        if(i != position):\n",
    "            target_vec = content_obj.item_matrix[i]\n",
    "            similarity = content_obj.cossine(content_vec, target_vec)\n",
    "            final_score = calculate_score(similarity, new_dict, i, df_mean, min_votes)\n",
    "            final_dict[i] = final_score\n",
    "    \n",
    "    recom = dict(sorted(final_dict.items(), key=lambda item: item[1], reverse=True))\n",
    "    final_recom = list(recom.keys())[:10]\n",
    "    print(final_recom)\n",
    "    recommendations = new_dataframe['title'].iloc[final_recom]\n",
    "    \n",
    "    return recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "180c1682",
   "metadata": {},
   "outputs": [],
   "source": [
    "position = new_dataframe[new_dataframe['title'] == \"The Lord of the Rings: The Fellowship of the Ring\"].index[0]\n",
    "content_vec = content_obj.item_matrix[position]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b40233aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def id_dict(new_dataframe, topic_vec):\n",
    "   new_dict = {}\n",
    "   for i in range(len(topic_vec)):\n",
    "        new_dict[new_dataframe['id'].iloc[i]] = topic_vec[i]\n",
    "    \n",
    "   return new_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5db012c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_idlist(new_dataframe):\n",
    "    id_list = []\n",
    "    for i in range(len(new_dataframe)):\n",
    "        id_list.append(new_dataframe['id'].iloc[i])\n",
    "    return id_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d8b649b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dict_fromdf(df):\n",
    "    new_dict = {}\n",
    "    for key, value in df.iteritems():     \n",
    "        l=[]\n",
    "        for x in value:\n",
    "            l.append(x)\n",
    "        new_dict[key] = tuple(l)\n",
    "    del new_dict['Unnamed: 0']\n",
    "    return new_dict"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
